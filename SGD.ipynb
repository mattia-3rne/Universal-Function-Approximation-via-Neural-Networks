{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "## Theoretical Approach\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization approach where the gradient is approximated using a **single training example** $(x^{(i)}, y^{(i)})$ chosen at random for each iteration. Unlike Batch Gradient Descent, which calculates the exact gradient by averaging over the entire dataset, SGD uses the gradient of the loss from one sample as an unbiased estimator of the true gradient. This introduces noise into the optimization path, causing the loss to fluctuate, but allows for significantly more frequent parameter updates.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### 1. Instantaneous Cost Function\n",
    "Instead of minimizing the average loss over the entire dataset, SGD considers the loss function for a specific, randomly selected sample $i$ at each step:\n",
    "\n",
    "$$\n",
    "J(\\theta; x^{(i)}, y^{(i)}) = \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2} (\\hat{y}^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "### 2. The Stochastic Gradient\n",
    "The gradient is computed with respect to the parameters $\\theta$ using only the $i$-th sample. Consequently, the parameters are updated immediately after processing this single sample, rather than waiting for the entire dataset to be evaluated. This removes the summation over $N$ found in batch methods:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\nabla_\\theta \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\n",
    "$$"
   ],
   "id": "53a1bb5406548690"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
