{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mini-Batch Gradient Descent (MBGD)\n",
    "\n",
    "## Theoretical Approach\n",
    "\n",
    "Mini-Batch Gradient Descent acts as a compromise between Batch Gradient Descent and Stochastic Gradient Descent. Instead of using the full dataset or a single sample, it partitions the training data into mini-batches of size $B$.\n",
    "\n",
    "The model computes the gradient and updates its parameters after processing each batch. This approach leverages the computational efficiency of matrix operations (like BGD) while providing the frequent updates and convergence speed of stochastic methods.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### 1. Mini-Batch Cost Function\n",
    "For a specific mini-batch $\\mathcal{B}$ containing $B$ samples, the cost function is the average loss over that specific subset:\n",
    "\n",
    "$$\n",
    "J_{\\mathcal{B}}(\\theta) = \\frac{1}{B} \\sum_{k=1}^{B} \\mathcal{L}(\\hat{y}^{(k)}, y^{(k)})\n",
    "$$\n",
    "\n",
    "### 2. The Mini-Batch Gradient\n",
    "The gradient is estimated by averaging the partial derivatives over the current mini-batch samples. This provides an approximation of the true gradient that is more stable than SGD but less computationally expensive than BGD:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{B} \\sum_{k=1}^{B} \\nabla_\\theta \\mathcal{L}(\\hat{y}^{(k)}, y^{(k)})\n",
    "$$"
   ],
   "id": "ae1012c3684b5472"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
